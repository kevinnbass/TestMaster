"""
Intelligent Code Optimizer - AI-Powered Code Optimization & Recommendation

This module implements advanced AI-powered code optimization that can suggest
improvements, performance enhancements, and quality improvements with expert-level
insight. It provides comprehensive optimization analysis, design pattern
recommendations, and intelligent refactoring suggestions.

Key Capabilities:
- AI-powered optimization suggestions with expert-level insight
- Performance analysis and bottleneck identification with recommendations
- Design pattern recommendations and architecture improvements
- Code quality enhancement with automated refactoring suggestions
- Security vulnerability detection and remediation
- Automated code review with expert-level feedback
- Learning from optimization outcomes to improve recommendations
- Integration with evolutionary code systems for autonomous optimization
"""

import asyncio
import logging
import ast
import json
import hashlib
import time
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional, Tuple, Union, Set, Callable
from dataclasses import dataclass, field
from enum import Enum
from pathlib import Path
import re
import importlib
import subprocess
from collections import defaultdict, Counter
import difflib

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class OptimizationType(Enum):
    """Types of code optimizations"""
    PERFORMANCE = "performance"
    MEMORY = "memory"
    READABILITY = "readability"
    MAINTAINABILITY = "maintainability"
    SECURITY = "security"
    TESTABILITY = "testability"
    ARCHITECTURE = "architecture"
    DESIGN_PATTERN = "design_pattern"
    ALGORITHM = "algorithm"
    CODE_QUALITY = "code_quality"

class OptimizationPriority(Enum):
    """Priority levels for optimizations"""
    CRITICAL = "critical"
    HIGH = "high"
    MEDIUM = "medium"
    LOW = "low"
    NICE_TO_HAVE = "nice_to_have"

class OptimizationStrategy(Enum):
    """Strategies for optimization implementation"""
    IMMEDIATE = "immediate"
    GRADUAL = "gradual"
    REFACTOR = "refactor"
    REDESIGN = "redesign"
    PATTERN_APPLICATION = "pattern_application"
    ALGORITHMIC_CHANGE = "algorithmic_change"

class RecommendationStatus(Enum):
    """Status of optimization recommendations"""
    PENDING = "pending"
    APPROVED = "approved"
    IMPLEMENTED = "implemented"
    REJECTED = "rejected"
    DEFERRED = "deferred"
    SUPERSEDED = "superseded"

@dataclass
class OptimizationRecommendation:
    """Represents an AI-generated optimization recommendation"""
    recommendation_id: str = field(default_factory=lambda: hashlib.md5(str(datetime.now()).encode()).hexdigest())
    optimization_type: OptimizationType = OptimizationType.PERFORMANCE
    priority: OptimizationPriority = OptimizationPriority.MEDIUM
    strategy: OptimizationStrategy = OptimizationStrategy.GRADUAL
    target_file: str = ""
    target_element: str = ""
    target_lines: Tuple[int, int] = (0, 0)
    description: str = ""
    reasoning: str = ""
    expected_improvement: Dict[str, float] = field(default_factory=dict)
    implementation_effort: str = ""
    risk_assessment: Dict[str, float] = field(default_factory=dict)
    original_code: str = ""
    optimized_code: str = ""
    alternative_solutions: List[str] = field(default_factory=list)
    prerequisites: List[str] = field(default_factory=list)
    testing_recommendations: List[str] = field(default_factory=list)
    documentation_updates: List[str] = field(default_factory=list)
    compatibility_impact: Dict[str, str] = field(default_factory=dict)
    confidence_score: float = 0.0
    learning_source: str = ""
    status: RecommendationStatus = RecommendationStatus.PENDING
    created_timestamp: datetime = field(default_factory=datetime.now)
    implementation_timestamp: Optional[datetime] = None
    validation_results: Dict[str, Any] = field(default_factory=dict)

@dataclass
class PerformanceAnalysis:
    """Performance analysis results"""
    analysis_id: str = field(default_factory=lambda: str(datetime.now().timestamp()))
    bottlenecks: List[Dict[str, Any]] = field(default_factory=list)
    complexity_analysis: Dict[str, Any] = field(default_factory=dict)
    memory_analysis: Dict[str, Any] = field(default_factory=dict)
    algorithmic_efficiency: Dict[str, str] = field(default_factory=dict)
    performance_score: float = 0.0
    optimization_potential: float = 0.0
    critical_paths: List[str] = field(default_factory=list)
    profiling_data: Dict[str, Any] = field(default_factory=dict)

@dataclass
class QualityAnalysis:
    """Code quality analysis results"""
    analysis_id: str = field(default_factory=lambda: str(datetime.now().timestamp()))
    quality_metrics: Dict[str, float] = field(default_factory=dict)
    maintainability_issues: List[Dict[str, Any]] = field(default_factory=list)
    readability_issues: List[Dict[str, Any]] = field(default_factory=list)
    design_violations: List[Dict[str, Any]] = field(default_factory=list)
    code_smells: List[Dict[str, Any]] = field(default_factory=list)
    technical_debt_items: List[Dict[str, Any]] = field(default_factory=list)
    refactoring_opportunities: List[Dict[str, Any]] = field(default_factory=list)
    overall_quality_score: float = 0.0

@dataclass
class DesignPatternRecommendation:
    """Design pattern application recommendation"""
    pattern_name: str = ""
    applicability_score: float = 0.0
    current_structure: str = ""
    pattern_structure: str = ""
    benefits: List[str] = field(default_factory=list)
    implementation_steps: List[str] = field(default_factory=list)
    code_examples: Dict[str, str] = field(default_factory=dict)
    impact_analysis: Dict[str, Any] = field(default_factory=dict)

class PerformanceAnalyzer:
    """Analyzes code performance and identifies bottlenecks"""
    
    def __init__(self):
        self.complexity_thresholds = {
            'low': 5,
            'medium': 10,
            'high': 20,
            'very_high': 50
        }
        
        self.performance_patterns = {
            'inefficient_loops': [
                r'for\s+\w+\s+in\s+range\(len\(',
                r'while.*len\(',
            ],
            'string_concatenation': [
                r'\+\s*=\s*["\']',
                r'["\'].*\+.*["\']'
            ],
            'repeated_calculations': [
                r'for.*in.*:.*\w+\(.*\)',
                r'while.*:.*\w+\(.*\)'
            ],
            'inefficient_data_structures': [
                r'\.append\(.*\)\s*in\s+for',
                r'list\(.*\)\s*\+\s*list\('
            ]
        }
    
    def analyze_performance(self, code: str, file_path: str = "") -> PerformanceAnalysis:
        """Comprehensive performance analysis of code"""
        try:
            analysis = PerformanceAnalysis()
            
            # Parse code
            try:
                tree = ast.parse(code)
            except SyntaxError:
                return analysis
            
            # Analyze complexity
            analysis.complexity_analysis = self._analyze_complexity(tree)
            
            # Identify bottlenecks
            analysis.bottlenecks = self._identify_bottlenecks(tree, code)
            
            # Analyze memory usage patterns
            analysis.memory_analysis = self._analyze_memory_patterns(tree, code)
            
            # Analyze algorithmic efficiency
            analysis.algorithmic_efficiency = self._analyze_algorithmic_efficiency(tree)
            
            # Calculate performance score
            analysis.performance_score = self._calculate_performance_score(analysis)
            
            # Estimate optimization potential
            analysis.optimization_potential = self._estimate_optimization_potential(analysis)
            
            # Identify critical paths
            analysis.critical_paths = self._identify_critical_paths(tree)
            
            return analysis
            
        except Exception as e:
            logger.error(f"Error analyzing performance: {e}")
            return PerformanceAnalysis()
    
    def _analyze_complexity(self, tree: ast.AST) -> Dict[str, Any]:
        """Analyze computational complexity of code"""
        try:
            complexity_info = {
                'cyclomatic_complexity': 0,
                'cognitive_complexity': 0,
                'nesting_depth': 0,
                'function_complexities': {},
                'class_complexities': {}
            }
            
            for node in ast.walk(tree):
                if isinstance(node, ast.FunctionDef):
                    func_complexity = self._calculate_function_complexity(node)
                    complexity_info['function_complexities'][node.name] = func_complexity
                    complexity_info['cyclomatic_complexity'] += func_complexity['cyclomatic']
                    complexity_info['cognitive_complexity'] += func_complexity['cognitive']
                    complexity_info['nesting_depth'] = max(
                        complexity_info['nesting_depth'], 
                        func_complexity['nesting_depth']
                    )
                elif isinstance(node, ast.ClassDef):
                    class_complexity = self._calculate_class_complexity(node)
                    complexity_info['class_complexities'][node.name] = class_complexity
            
            return complexity_info
            
        except Exception as e:
            logger.error(f"Error analyzing complexity: {e}")
            return {}
    
    def _calculate_function_complexity(self, node: ast.FunctionDef) -> Dict[str, Any]:
        """Calculate complexity metrics for a function"""
        try:
            cyclomatic = 1  # Base complexity
            cognitive = 0
            max_nesting = 0
            
            def analyze_node(n, nesting_level=0):
                nonlocal cyclomatic, cognitive, max_nesting
                max_nesting = max(max_nesting, nesting_level)
                
                if isinstance(n, (ast.If, ast.For, ast.While, ast.Try)):
                    cyclomatic += 1
                    cognitive += 1 + nesting_level  # Cognitive complexity increases with nesting
                    
                    for child in ast.iter_child_nodes(n):
                        analyze_node(child, nesting_level + 1)
                elif isinstance(n, ast.BoolOp):
                    cyclomatic += len(n.values) - 1
                    cognitive += len(n.values) - 1
                else:
                    for child in ast.iter_child_nodes(n):
                        analyze_node(child, nesting_level)
            
            analyze_node(node)
            
            return {
                'cyclomatic': cyclomatic,
                'cognitive': cognitive,
                'nesting_depth': max_nesting,
                'line_count': (node.end_lineno or node.lineno) - node.lineno,
                'parameter_count': len(node.args.args)
            }
            
        except Exception as e:
            logger.error(f"Error calculating function complexity: {e}")
            return {'cyclomatic': 1, 'cognitive': 1}
    
    def _calculate_class_complexity(self, node: ast.ClassDef) -> Dict[str, Any]:
        """Calculate complexity metrics for a class"""
        try:
            method_complexities = []
            total_methods = 0
            
            for child in node.body:
                if isinstance(child, ast.FunctionDef):
                    total_methods += 1
                    method_complexity = self._calculate_function_complexity(child)
                    method_complexities.append(method_complexity['cyclomatic'])
            
            avg_complexity = np.mean(method_complexities) if method_complexities else 0
            max_complexity = max(method_complexities) if method_complexities else 0
            
            return {
                'method_count': total_methods,
                'average_method_complexity': avg_complexity,
                'max_method_complexity': max_complexity,
                'total_complexity': sum(method_complexities),
                'inheritance_depth': len(node.bases)
            }
            
        except Exception as e:
            logger.error(f"Error calculating class complexity: {e}")
            return {'method_count': 0}
    
    def _identify_bottlenecks(self, tree: ast.AST, code: str) -> List[Dict[str, Any]]:
        """Identify performance bottlenecks in code"""
        try:
            bottlenecks = []
            
            # Check for performance anti-patterns
            for pattern_type, patterns in self.performance_patterns.items():
                for pattern in patterns:
                    matches = re.finditer(pattern, code, re.MULTILINE)
                    for match in matches:
                        line_num = code[:match.start()].count('\n') + 1
                        bottlenecks.append({
                            'type': pattern_type,
                            'description': f"{pattern_type.replace('_', ' ').title()} detected",
                            'line': line_num,
                            'code_snippet': match.group(),
                            'severity': self._get_bottleneck_severity(pattern_type),
                            'optimization_suggestion': self._get_optimization_suggestion(pattern_type)
                        })
            
            # Check for nested loops
            for node in ast.walk(tree):
                if isinstance(node, ast.For):
                    nesting_level = self._calculate_loop_nesting(node)
                    if nesting_level > 2:
                        bottlenecks.append({
                            'type': 'nested_loops',
                            'description': f"Deeply nested loops (level {nesting_level})",
                            'line': node.lineno,
                            'severity': 'high',
                            'optimization_suggestion': 'Consider algorithmic improvements or vectorization'
                        })
            
            return bottlenecks
            
        except Exception as e:
            logger.error(f"Error identifying bottlenecks: {e}")
            return []
    
    def _calculate_loop_nesting(self, node: ast.For) -> int:
        """Calculate nesting level of loops"""
        try:
            nesting = 1
            for child in ast.walk(node):
                if isinstance(child, (ast.For, ast.While)) and child != node:
                    child_nesting = 1 + self._calculate_loop_nesting(child)
                    nesting = max(nesting, child_nesting)
            return nesting
        except:
            return 1
    
    def _get_bottleneck_severity(self, pattern_type: str) -> str:
        """Get severity level for bottleneck type"""
        severity_map = {
            'inefficient_loops': 'high',
            'string_concatenation': 'medium',
            'repeated_calculations': 'high',
            'inefficient_data_structures': 'medium'
        }
        return severity_map.get(pattern_type, 'low')
    
    def _get_optimization_suggestion(self, pattern_type: str) -> str:
        """Get optimization suggestion for bottleneck type"""
        suggestions = {
            'inefficient_loops': 'Use enumerate() or list comprehensions',
            'string_concatenation': 'Use join() or f-strings for better performance',
            'repeated_calculations': 'Cache results or move calculations outside loops',
            'inefficient_data_structures': 'Use appropriate data structures (sets, deques, etc.)'
        }
        return suggestions.get(pattern_type, 'Consider refactoring for better performance')
    
    def _analyze_memory_patterns(self, tree: ast.AST, code: str) -> Dict[str, Any]:
        """Analyze memory usage patterns"""
        try:
            memory_info = {
                'large_data_structures': [],
                'memory_leaks_potential': [],
                'inefficient_copying': [],
                'memory_score': 0.8
            }
            
            # Check for large list operations
            for node in ast.walk(tree):
                if isinstance(node, ast.ListComp):
                    # List comprehensions can use significant memory
                    memory_info['large_data_structures'].append({
                        'type': 'list_comprehension',
                        'line': node.lineno,
                        'suggestion': 'Consider generator expressions for large datasets'
                    })
                elif isinstance(node, ast.Call):
                    if isinstance(node.func, ast.Name) and node.func.id in ['list', 'dict', 'set']:
                        memory_info['large_data_structures'].append({
                            'type': 'explicit_construction',
                            'line': node.lineno,
                            'suggestion': 'Consider lazy evaluation or streaming'
                        })
            
            # Check for potential memory leaks (simplified)
            if 'global ' in code or 'class ' in code:
                memory_info['memory_leaks_potential'].append({
                    'type': 'global_state',
                    'suggestion': 'Review global variables and class instances for proper cleanup'
                })
            
            return memory_info
            
        except Exception as e:
            logger.error(f"Error analyzing memory patterns: {e}")
            return {}
    
    def _analyze_algorithmic_efficiency(self, tree: ast.AST) -> Dict[str, str]:
        """Analyze algorithmic efficiency of code"""
        try:
            efficiency_analysis = {}
            
            for node in ast.walk(tree):
                if isinstance(node, ast.FunctionDef):
                    efficiency = self._estimate_time_complexity(node)
                    efficiency_analysis[node.name] = efficiency
            
            return efficiency_analysis
            
        except Exception as e:
            logger.error(f"Error analyzing algorithmic efficiency: {e}")
            return {}
    
    def _estimate_time_complexity(self, node: ast.FunctionDef) -> str:
        """Estimate time complexity of function"""
        try:
            # Simple heuristic-based complexity estimation
            loop_count = 0
            nested_loops = 0
            
            for child in ast.walk(node):
                if isinstance(child, (ast.For, ast.While)):
                    loop_count += 1
                    # Check for nested loops
                    for nested in ast.walk(child):
                        if isinstance(nested, (ast.For, ast.While)) and nested != child:
                            nested_loops += 1
            
            if nested_loops > 1:
                return "O(n³) or higher"
            elif nested_loops == 1:
                return "O(n²)"
            elif loop_count > 0:
                return "O(n)"
            else:
                return "O(1)"
                
        except Exception as e:
            logger.error(f"Error estimating time complexity: {e}")
            return "O(?)"
    
    def _calculate_performance_score(self, analysis: PerformanceAnalysis) -> float:
        """Calculate overall performance score"""
        try:
            score = 1.0
            
            # Penalize high complexity
            avg_complexity = analysis.complexity_analysis.get('cyclomatic_complexity', 0) / max(1, len(analysis.complexity_analysis.get('function_complexities', {})))
            if avg_complexity > 10:
                score -= 0.3
            elif avg_complexity > 5:
                score -= 0.1
            
            # Penalize bottlenecks
            high_severity_bottlenecks = len([b for b in analysis.bottlenecks if b.get('severity') == 'high'])
            score -= high_severity_bottlenecks * 0.2
            
            medium_severity_bottlenecks = len([b for b in analysis.bottlenecks if b.get('severity') == 'medium'])
            score -= medium_severity_bottlenecks * 0.1
            
            return max(0.0, score)
            
        except Exception as e:
            logger.error(f"Error calculating performance score: {e}")
            return 0.5
    
    def _estimate_optimization_potential(self, analysis: PerformanceAnalysis) -> float:
        """Estimate potential for performance optimization"""
        try:
            potential = 0.0
            
            # Higher potential with more bottlenecks
            potential += len(analysis.bottlenecks) * 0.1
            
            # Higher potential with high complexity
            avg_complexity = analysis.complexity_analysis.get('cyclomatic_complexity', 0)
            if avg_complexity > 20:
                potential += 0.4
            elif avg_complexity > 10:
                potential += 0.2
            
            return min(1.0, potential)
            
        except Exception as e:
            logger.error(f"Error estimating optimization potential: {e}")
            return 0.3
    
    def _identify_critical_paths(self, tree: ast.AST) -> List[str]:
        """Identify critical execution paths"""
        try:
            critical_paths = []
            
            for node in ast.walk(tree):
                if isinstance(node, ast.FunctionDef):
                    # Functions with high complexity are critical
                    complexity = self._calculate_function_complexity(node)
                    if complexity['cyclomatic'] > 10:
                        critical_paths.append(f"Function: {node.name}")
                        
                elif isinstance(node, ast.For):
                    # Nested loops are critical
                    nesting = self._calculate_loop_nesting(node)
                    if nesting > 2:
                        critical_paths.append(f"Nested loop at line {node.lineno}")
            
            return critical_paths
            
        except Exception as e:
            logger.error(f"Error identifying critical paths: {e}")
            return []

class QualityAnalyzer:
    """Analyzes code quality and identifies improvement opportunities"""
    
    def __init__(self):
        self.quality_patterns = {
            'long_methods': {'threshold': 50, 'weight': 0.3},
            'long_classes': {'threshold': 500, 'weight': 0.2},
            'too_many_parameters': {'threshold': 5, 'weight': 0.2},
            'deep_nesting': {'threshold': 4, 'weight': 0.3},
            'duplicated_code': {'threshold': 10, 'weight': 0.4}
        }
        
        self.code_smells = {
            'god_class': r'class\s+\w+.*:\s*\n(?:\s*.*\n){200,}',
            'feature_envy': r'self\.\w+\.\w+\.\w+',
            'data_clumps': r'def\s+\w+\([^)]*,\s*[^)]*,\s*[^)]*,\s*[^)]*,',
            'shotgun_surgery': r'import\s+\w+.*\n(?:.*import.*\n){5,}'
        }
    
    def analyze_quality(self, code: str, file_path: str = "") -> QualityAnalysis:
        """Comprehensive code quality analysis"""
        try:
            analysis = QualityAnalysis()
            
            # Parse code
            try:
                tree = ast.parse(code)
            except SyntaxError:
                return analysis
            
            # Analyze quality metrics
            analysis.quality_metrics = self._calculate_quality_metrics(tree, code)
            
            # Identify maintainability issues
            analysis.maintainability_issues = self._identify_maintainability_issues(tree, code)
            
            # Identify readability issues
            analysis.readability_issues = self._identify_readability_issues(tree, code)
            
            # Detect design violations
            analysis.design_violations = self._detect_design_violations(tree, code)
            
            # Detect code smells
            analysis.code_smells = self._detect_code_smells(tree, code)
            
            # Identify technical debt
            analysis.technical_debt_items = self._identify_technical_debt(tree, code)
            
            # Find refactoring opportunities
            analysis.refactoring_opportunities = self._find_refactoring_opportunities(tree, code)
            
            # Calculate overall quality score
            analysis.overall_quality_score = self._calculate_overall_quality_score(analysis)
            
            return analysis
            
        except Exception as e:
            logger.error(f"Error analyzing code quality: {e}")
            return QualityAnalysis()
    
    def _calculate_quality_metrics(self, tree: ast.AST, code: str) -> Dict[str, float]:
        """Calculate various quality metrics"""
        try:
            metrics = {}
            
            lines = code.split('\n')
            metrics['total_lines'] = len(lines)
            metrics['code_lines'] = len([l for l in lines if l.strip() and not l.strip().startswith('#')])
            metrics['comment_lines'] = len([l for l in lines if l.strip().startswith('#')])
            metrics['blank_lines'] = len([l for l in lines if not l.strip()])
            
            # Comment ratio
            metrics['comment_ratio'] = metrics['comment_lines'] / max(1, metrics['total_lines'])
            
            # Function and class counts
            metrics['function_count'] = len([n for n in ast.walk(tree) if isinstance(n, ast.FunctionDef)])
            metrics['class_count'] = len([n for n in ast.walk(tree) if isinstance(n, ast.ClassDef)])
            
            # Average function length
            function_lengths = []
            for node in ast.walk(tree):
                if isinstance(node, ast.FunctionDef):
                    length = (node.end_lineno or node.lineno) - node.lineno
                    function_lengths.append(length)
            
            metrics['avg_function_length'] = np.mean(function_lengths) if function_lengths else 0
            metrics['max_function_length'] = max(function_lengths) if function_lengths else 0
            
            return metrics
            
        except Exception as e:
            logger.error(f"Error calculating quality metrics: {e}")
            return {}
    
    def _identify_maintainability_issues(self, tree: ast.AST, code: str) -> List[Dict[str, Any]]:
        """Identify maintainability issues"""
        try:
            issues = []
            
            # Long methods
            for node in ast.walk(tree):
                if isinstance(node, ast.FunctionDef):
                    length = (node.end_lineno or node.lineno) - node.lineno
                    if length > self.quality_patterns['long_methods']['threshold']:
                        issues.append({
                            'type': 'long_method',
                            'description': f"Method '{node.name}' is {length} lines long",
                            'severity': 'medium',
                            'line': node.lineno,
                            'suggestion': 'Consider breaking into smaller methods'
                        })
                
                # Too many parameters
                if isinstance(node, ast.FunctionDef):
                    param_count = len(node.args.args)
                    if param_count > self.quality_patterns['too_many_parameters']['threshold']:
                        issues.append({
                            'type': 'too_many_parameters',
                            'description': f"Method '{node.name}' has {param_count} parameters",
                            'severity': 'medium',
                            'line': node.lineno,
                            'suggestion': 'Consider using parameter objects or builder pattern'
                        })
            
            # Long classes
            for node in ast.walk(tree):
                if isinstance(node, ast.ClassDef):
                    length = (node.end_lineno or node.lineno) - node.lineno
                    if length > self.quality_patterns['long_classes']['threshold']:
                        issues.append({
                            'type': 'long_class',
                            'description': f"Class '{node.name}' is {length} lines long",
                            'severity': 'high',
                            'line': node.lineno,
                            'suggestion': 'Consider decomposing into smaller classes'
                        })
            
            return issues
            
        except Exception as e:
            logger.error(f"Error identifying maintainability issues: {e}")
            return []
    
    def _identify_readability_issues(self, tree: ast.AST, code: str) -> List[Dict[str, Any]]:
        """Identify readability issues"""
        try:
            issues = []
            
            lines = code.split('\n')
            
            # Long lines
            for i, line in enumerate(lines):
                if len(line) > 120:  # PEP 8 recommends 79, but 120 is common
                    issues.append({
                        'type': 'long_line',
                        'description': f"Line {i+1} is {len(line)} characters long",
                        'severity': 'low',
                        'line': i + 1,
                        'suggestion': 'Break long lines for better readability'
                    })
            
            # Poor variable names
            for node in ast.walk(tree):
                if isinstance(node, ast.Name):
                    name = node.id
                    if len(name) <= 2 and name not in ['i', 'j', 'k', 'x', 'y', 'z']:
                        issues.append({
                            'type': 'poor_variable_name',
                            'description': f"Variable '{name}' has unclear name",
                            'severity': 'low',
                            'line': getattr(node, 'lineno', 0),
                            'suggestion': 'Use descriptive variable names'
                        })
            
            # Missing docstrings
            for node in ast.walk(tree):
                if isinstance(node, (ast.FunctionDef, ast.ClassDef)):
                    if not ast.get_docstring(node):
                        issues.append({
                            'type': 'missing_docstring',
                            'description': f"{type(node).__name__[:-3]} '{node.name}' missing docstring",
                            'severity': 'medium',
                            'line': node.lineno,
                            'suggestion': 'Add docstring to document purpose and usage'
                        })
            
            return issues
            
        except Exception as e:
            logger.error(f"Error identifying readability issues: {e}")
            return []
    
    def _detect_design_violations(self, tree: ast.AST, code: str) -> List[Dict[str, Any]]:
        """Detect design principle violations"""
        try:
            violations = []
            
            # Single Responsibility Principle violations (simplified)
            for node in ast.walk(tree):
                if isinstance(node, ast.ClassDef):
                    method_count = len([n for n in node.body if isinstance(n, ast.FunctionDef)])
                    if method_count > 15:  # Arbitrary threshold
                        violations.append({
                            'type': 'srp_violation',
                            'description': f"Class '{node.name}' has {method_count} methods",
                            'severity': 'medium',
                            'line': node.lineno,
                            'suggestion': 'Consider splitting class responsibilities'
                        })
            
            # Open/Closed Principle violations (detect switch statements)
            for node in ast.walk(tree):
                if isinstance(node, ast.If):
                    elif_count = 0
                    current = node
                    while hasattr(current, 'orelse') and current.orelse:
                        if len(current.orelse) == 1 and isinstance(current.orelse[0], ast.If):
                            elif_count += 1
                            current = current.orelse[0]
                        else:
                            break
                    
                    if elif_count > 5:  # Many elif statements suggest switch-like behavior
                        violations.append({
                            'type': 'ocp_violation',
                            'description': f"Long if-elif chain with {elif_count} branches",
                            'severity': 'medium',
                            'line': node.lineno,
                            'suggestion': 'Consider using polymorphism or strategy pattern'
                        })
            
            return violations
            
        except Exception as e:
            logger.error(f"Error detecting design violations: {e}")
            return []
    
    def _detect_code_smells(self, tree: ast.AST, code: str) -> List[Dict[str, Any]]:
        """Detect code smells using pattern matching"""
        try:
            smells = []
            
            for smell_type, pattern in self.code_smells.items():
                matches = re.finditer(pattern, code, re.MULTILINE | re.DOTALL)
                for match in matches:
                    line_num = code[:match.start()].count('\n') + 1
                    smells.append({
                        'type': smell_type,
                        'description': f"{smell_type.replace('_', ' ').title()} detected",
                        'severity': self._get_smell_severity(smell_type),
                        'line': line_num,
                        'suggestion': self._get_smell_suggestion(smell_type)
                    })
            
            # Detect duplicate code (simplified)
            lines = code.split('\n')
            line_groups = defaultdict(list)
            for i, line in enumerate(lines):
                stripped = line.strip()
                if stripped and not stripped.startswith('#') and len(stripped) > 10:
                    line_groups[stripped].append(i + 1)
            
            for line_content, line_numbers in line_groups.items():
                if len(line_numbers) > 2:  # Same line appears more than twice
                    smells.append({
                        'type': 'duplicate_code',
                        'description': f"Duplicate line found {len(line_numbers)} times",
                        'severity': 'medium',
                        'line': line_numbers[0],
                        'suggestion': 'Extract common code into a method'
                    })
            
            return smells
            
        except Exception as e:
            logger.error(f"Error detecting code smells: {e}")
            return []
    
    def _get_smell_severity(self, smell_type: str) -> str:
        """Get severity for code smell type"""
        severity_map = {
            'god_class': 'high',
            'feature_envy': 'medium',
            'data_clumps': 'medium',
            'shotgun_surgery': 'high',
            'duplicate_code': 'medium'
        }
        return severity_map.get(smell_type, 'low')
    
    def _get_smell_suggestion(self, smell_type: str) -> str:
        """Get suggestion for code smell type"""
        suggestions = {
            'god_class': 'Break large class into smaller, focused classes',
            'feature_envy': 'Move method to the class it envies',
            'data_clumps': 'Group related parameters into objects',
            'shotgun_surgery': 'Consolidate related changes',
            'duplicate_code': 'Extract common code into reusable methods'
        }
        return suggestions.get(smell_type, 'Consider refactoring to improve design')
    
    def _identify_technical_debt(self, tree: ast.AST, code: str) -> List[Dict[str, Any]]:
        """Identify technical debt items"""
        try:
            debt_items = []
            
            # TODO/FIXME comments
            todo_pattern = r'#.*(?:TODO|FIXME|HACK|XXX).*'
            matches = re.finditer(todo_pattern, code, re.IGNORECASE)
            for match in matches:
                line_num = code[:match.start()].count('\n') + 1
                debt_items.append({
                    'type': 'todo_comment',
                    'description': match.group().strip(),
                    'severity': 'low',
                    'line': line_num,
                    'suggestion': 'Address TODO item or create issue for tracking'
                })
            
            # Magic numbers
            magic_number_pattern = r'\b(?<![\w.])\d{2,}\b(?![\w.])'
            matches = re.finditer(magic_number_pattern, code)
            for match in matches:
                line_num = code[:match.start()].count('\n') + 1
                debt_items.append({
                    'type': 'magic_number',
                    'description': f"Magic number '{match.group()}' found",
                    'severity': 'low',
                    'line': line_num,
                    'suggestion': 'Replace with named constant'
                })
            
            return debt_items
            
        except Exception as e:
            logger.error(f"Error identifying technical debt: {e}")
            return []
    
    def _find_refactoring_opportunities(self, tree: ast.AST, code: str) -> List[Dict[str, Any]]:
        """Find opportunities for refactoring"""
        try:
            opportunities = []
            
            # Extract method opportunities
            for node in ast.walk(tree):
                if isinstance(node, ast.FunctionDef):
                    length = (node.end_lineno or node.lineno) - node.lineno
                    if length > 30:  # Long methods can be extracted
                        opportunities.append({
                            'type': 'extract_method',
                            'description': f"Long method '{node.name}' ({length} lines)",
                            'priority': 'medium',
                            'line': node.lineno,
                            'suggestion': 'Extract logical blocks into separate methods'
                        })
            
            # Extract class opportunities
            for node in ast.walk(tree):
                if isinstance(node, ast.ClassDef):
                    method_count = len([n for n in node.body if isinstance(n, ast.FunctionDef)])
                    if method_count > 20:
                        opportunities.append({
                            'type': 'extract_class',
                            'description': f"Large class '{node.name}' ({method_count} methods)",
                            'priority': 'high',
                            'line': node.lineno,
                            'suggestion': 'Extract related methods into separate classes'
                        })
            
            return opportunities
            
        except Exception as e:
            logger.error(f"Error finding refactoring opportunities: {e}")
            return []
    
    def _calculate_overall_quality_score(self, analysis: QualityAnalysis) -> float:
        """Calculate overall quality score"""
        try:
            score = 1.0
            
            # Penalize based on issues
            issue_weights = {
                'high': 0.3,
                'medium': 0.2,
                'low': 0.1
            }
            
            all_issues = (analysis.maintainability_issues + 
                         analysis.readability_issues + 
                         analysis.design_violations + 
                         analysis.code_smells)
            
            for issue in all_issues:
                severity = issue.get('severity', 'low')
                score -= issue_weights.get(severity, 0.1)
            
            # Bonus for good practices
            comment_ratio = analysis.quality_metrics.get('comment_ratio', 0)
            if comment_ratio > 0.1:  # Good commenting
                score += 0.1
            
            return max(0.0, min(1.0, score))
            
        except Exception as e:
            logger.error(f"Error calculating quality score: {e}")
            return 0.5

class DesignPatternRecommendationEngine:
    """Recommends appropriate design patterns for code structures"""
    
    def __init__(self):
        self.pattern_signatures = {
            'singleton': {
                'indicators': ['__new__', '_instance', 'instance'],
                'structure': 'single instance management',
                'benefits': ['controlled instantiation', 'global access point']
            },
            'factory': {
                'indicators': ['create', 'build', 'make'],
                'structure': 'object creation abstraction',
                'benefits': ['loose coupling', 'centralized creation logic']
            },
            'observer': {
                'indicators': ['notify', 'update', 'subscribe', 'listener'],
                'structure': 'event notification system',
                'benefits': ['loose coupling', 'dynamic relationships']
            },
            'strategy': {
                'indicators': ['algorithm', 'method', 'strategy'],
                'structure': 'algorithm encapsulation',
                'benefits': ['algorithm interchange', 'open/closed principle']
            },
            'command': {
                'indicators': ['execute', 'undo', 'command'],
                'structure': 'request encapsulation',
                'benefits': ['request queuing', 'undo functionality']
            }
        }
    
    def recommend_patterns(self, tree: ast.AST, code: str) -> List[DesignPatternRecommendation]:
        """Recommend design patterns for code structure"""
        try:
            recommendations = []
            
            # Analyze code structure
            for pattern_name, signature in self.pattern_signatures.items():
                applicability = self._assess_pattern_applicability(
                    tree, code, pattern_name, signature
                )
                
                if applicability > 0.3:  # Threshold for recommendation
                    recommendation = DesignPatternRecommendation(
                        pattern_name=pattern_name,
                        applicability_score=applicability,
                        current_structure=self._describe_current_structure(tree, pattern_name),
                        pattern_structure=signature['structure'],
                        benefits=signature['benefits'],
                        implementation_steps=self._get_implementation_steps(pattern_name),
                        code_examples=self._get_code_examples(pattern_name),
                        impact_analysis=self._analyze_pattern_impact(tree, pattern_name)
                    )
                    recommendations.append(recommendation)
            
            # Sort by applicability score
            recommendations.sort(key=lambda r: r.applicability_score, reverse=True)
            
            return recommendations
            
        except Exception as e:
            logger.error(f"Error recommending patterns: {e}")
            return []
    
    def _assess_pattern_applicability(self, tree: ast.AST, code: str, 
                                    pattern_name: str, signature: Dict[str, Any]) -> float:
        """Assess how applicable a pattern is to the current code"""
        try:
            score = 0.0
            
            # Check for pattern indicators in code
            indicators = signature.get('indicators', [])
            for indicator in indicators:
                if indicator.lower() in code.lower():
                    score += 0.2
            
            # Pattern-specific analysis
            if pattern_name == 'singleton':
                # Look for global state or single instance patterns
                if '__new__' in code or '_instance' in code:
                    score += 0.4
                    
            elif pattern_name == 'factory':
                # Look for object creation patterns
                creation_methods = len([n for n in ast.walk(tree) 
                                      if isinstance(n, ast.FunctionDef) 
                                      and any(word in n.name.lower() 
                                            for word in ['create', 'build', 'make'])])
                if creation_methods > 0:
                    score += 0.3
                    
            elif pattern_name == 'observer':
                # Look for event/notification patterns
                if any(word in code.lower() for word in ['notify', 'subscribe', 'event']):
                    score += 0.4
                    
            elif pattern_name == 'strategy':
                # Look for algorithm variations
                if_elif_chains = self._count_if_elif_chains(tree)
                if if_elif_chains > 2:
                    score += 0.3
                    
            elif pattern_name == 'command':
                # Look for command-like structures
                if 'execute' in code.lower() or 'command' in code.lower():
                    score += 0.3
            
            return min(1.0, score)
            
        except Exception as e:
            logger.error(f"Error assessing pattern applicability: {e}")
            return 0.0
    
    def _count_if_elif_chains(self, tree: ast.AST) -> int:
        """Count if-elif chains that might benefit from strategy pattern"""
        try:
            chains = 0
            
            for node in ast.walk(tree):
                if isinstance(node, ast.If):
                    elif_count = 0
                    current = node
                    while hasattr(current, 'orelse') and current.orelse:
                        if len(current.orelse) == 1 and isinstance(current.orelse[0], ast.If):
                            elif_count += 1
                            current = current.orelse[0]
                        else:
                            break
                    
                    if elif_count > 2:
                        chains += 1
            
            return chains
            
        except Exception as e:
            logger.error(f"Error counting if-elif chains: {e}")
            return 0
    
    def _describe_current_structure(self, tree: ast.AST, pattern_name: str) -> str:
        """Describe current code structure"""
        try:
            if pattern_name == 'singleton':
                return "Multiple potential instantiation points"
            elif pattern_name == 'factory':
                return "Direct object instantiation in client code"
            elif pattern_name == 'observer':
                return "Direct coupling between components"
            elif pattern_name == 'strategy':
                return "Conditional logic for algorithm selection"
            elif pattern_name == 'command':
                return "Direct method calls without abstraction"
            else:
                return "Current procedural structure"
        except:
            return "Current structure analysis unavailable"
    
    def _get_implementation_steps(self, pattern_name: str) -> List[str]:
        """Get implementation steps for pattern"""
        steps_map = {
            'singleton': [
                "Create private class variable for instance",
                "Override __new__ method",
                "Implement instance checking logic",
                "Ensure thread safety if needed"
            ],
            'factory': [
                "Create abstract factory interface",
                "Implement concrete factory classes",
                "Replace direct instantiation with factory calls",
                "Add product hierarchy if needed"
            ],
            'observer': [
                "Define observer interface",
                "Implement subject (observable) class",
                "Create concrete observer classes",
                "Add subscription/notification mechanisms"
            ],
            'strategy': [
                "Define strategy interface",
                "Implement concrete strategy classes",
                "Add context class to use strategies",
                "Replace conditional logic with strategy selection"
            ],
            'command': [
                "Define command interface",
                "Implement concrete command classes",
                "Create invoker class",
                "Add undo functionality if needed"
            ]
        }
        return steps_map.get(pattern_name, ["Pattern-specific implementation steps"])
    
    def _get_code_examples(self, pattern_name: str) -> Dict[str, str]:
        """Get code examples for pattern"""
        examples = {
            'singleton': {
                'before': '''
class Database:
    def __init__(self):
        self.connection = create_connection()

# Multiple instances created
db1 = Database()
db2 = Database()
''',
                'after': '''
class Database:
    _instance = None
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
            cls._instance.connection = create_connection()
        return cls._instance
'''
            },
            'factory': {
                'before': '''
if vehicle_type == 'car':
    return Car()
elif vehicle_type == 'truck':
    return Truck()
''',
                'after': '''
class VehicleFactory:
    @staticmethod
    def create_vehicle(vehicle_type):
        if vehicle_type == 'car':
            return Car()
        elif vehicle_type == 'truck':
            return Truck()
'''
            }
        }
        return examples.get(pattern_name, {})
    
    def _analyze_pattern_impact(self, tree: ast.AST, pattern_name: str) -> Dict[str, Any]:
        """Analyze impact of applying pattern"""
        try:
            impact = {
                'complexity_change': 'medium',
                'maintainability_change': 'improved',
                'performance_impact': 'minimal',
                'testing_impact': 'requires_new_tests'
            }
            
            # Pattern-specific impact analysis
            if pattern_name == 'singleton':
                impact['testing_impact'] = 'may_complicate_testing'
                impact['concurrency_impact'] = 'requires_thread_safety'
            elif pattern_name == 'factory':
                impact['complexity_change'] = 'increased_initially'
                impact['extensibility'] = 'highly_improved'
            
            return impact
            
        except Exception as e:
            logger.error(f"Error analyzing pattern impact: {e}")
            return {}

class IntelligentCodeOptimizer:
    """Master intelligent code optimizer with AI-powered recommendations"""
    
    def __init__(self):
        self.performance_analyzer = PerformanceAnalyzer()
        self.quality_analyzer = QualityAnalyzer()
        self.pattern_engine = DesignPatternRecommendationEngine()
        self.optimization_history = []
        self.learning_database = {}
        
        # Configuration
        self.enable_performance_analysis = True
        self.enable_quality_analysis = True
        self.enable_pattern_recommendations = True
        self.enable_learning = True
        self.confidence_threshold = 0.7
        
        logger.info("Intelligent Code Optimizer initialized")
    
    async def analyze_and_optimize(self, file_path: str, 
                                 focus_areas: List[OptimizationType] = None) -> List[OptimizationRecommendation]:
        """Comprehensive analysis and optimization recommendations"""
        try:
            # Read and parse code
            with open(file_path, 'r', encoding='utf-8') as f:
                code = f.read()
            
            try:
                tree = ast.parse(code)
            except SyntaxError as e:
                logger.error(f"Syntax error in {file_path}: {e}")
                return []
            
            recommendations = []
            
            # Performance analysis
            if (not focus_areas or OptimizationType.PERFORMANCE in focus_areas) and self.enable_performance_analysis:
                perf_recommendations = await self._generate_performance_recommendations(
                    tree, code, file_path
                )
                recommendations.extend(perf_recommendations)
            
            # Quality analysis
            if (not focus_areas or OptimizationType.CODE_QUALITY in focus_areas) and self.enable_quality_analysis:
                quality_recommendations = await self._generate_quality_recommendations(
                    tree, code, file_path
                )
                recommendations.extend(quality_recommendations)
            
            # Design pattern recommendations
            if (not focus_areas or OptimizationType.DESIGN_PATTERN in focus_areas) and self.enable_pattern_recommendations:
                pattern_recommendations = await self._generate_pattern_recommendations(
                    tree, code, file_path
                )
                recommendations.extend(pattern_recommendations)
            
            # Security analysis
            if not focus_areas or OptimizationType.SECURITY in focus_areas:
                security_recommendations = await self._generate_security_recommendations(
                    tree, code, file_path
                )
                recommendations.extend(security_recommendations)
            
            # Sort by priority and confidence
            recommendations.sort(key=lambda r: (r.priority.value, -r.confidence_score))
            
            # Filter by confidence threshold
            recommendations = [r for r in recommendations if r.confidence_score >= self.confidence_threshold]
            
            # Learn from recommendations if enabled
            if self.enable_learning:
                await self._update_learning_database(recommendations, file_path)
            
            logger.info(f"Generated {len(recommendations)} optimization recommendations for {file_path}")
            return recommendations
            
        except Exception as e:
            logger.error(f"Error analyzing and optimizing {file_path}: {e}")
            return []
    
    async def _generate_performance_recommendations(self, tree: ast.AST, code: str, 
                                                  file_path: str) -> List[OptimizationRecommendation]:
        """Generate performance optimization recommendations"""
        try:
            recommendations = []
            
            # Analyze performance
            perf_analysis = self.performance_analyzer.analyze_performance(code, file_path)
            
            # Generate recommendations for bottlenecks
            for bottleneck in perf_analysis.bottlenecks:
                recommendation = OptimizationRecommendation(
                    optimization_type=OptimizationType.PERFORMANCE,
                    priority=OptimizationPriority.HIGH if bottleneck['severity'] == 'high' else OptimizationPriority.MEDIUM,
                    strategy=OptimizationStrategy.IMMEDIATE,
                    target_file=file_path,
                    target_lines=(bottleneck.get('line', 0), bottleneck.get('line', 0)),
                    description=bottleneck['description'],
                    reasoning=f"Performance bottleneck detected: {bottleneck['type']}",
                    expected_improvement={'performance': 0.2, 'readability': 0.1},
                    implementation_effort="Low to Medium",
                    risk_assessment={'performance_risk': 0.1, 'compatibility_risk': 0.05},
                    original_code=bottleneck.get('code_snippet', ''),
                    optimized_code=await self._generate_optimized_code(bottleneck),
                    confidence_score=0.8,
                    learning_source="performance_pattern_analysis"
                )
                recommendations.append(recommendation)
            
            # Generate recommendations for high complexity functions
            for func_name, complexity in perf_analysis.complexity_analysis.get('function_complexities', {}).items():
                if complexity['cyclomatic'] > 15:
                    recommendation = OptimizationRecommendation(
                        optimization_type=OptimizationType.MAINTAINABILITY,
                        priority=OptimizationPriority.MEDIUM,
                        strategy=OptimizationStrategy.REFACTOR,
                        target_file=file_path,
                        target_element=func_name,
                        description=f"High complexity function: {func_name} (complexity: {complexity['cyclomatic']})",
                        reasoning="High cyclomatic complexity affects maintainability and testing",
                        expected_improvement={'maintainability': 0.3, 'testability': 0.25},
                        implementation_effort="Medium",
                        risk_assessment={'maintainability_risk': 0.1},
                        confidence_score=0.75,
                        learning_source="complexity_analysis"
                    )
                    recommendation.alternative_solutions = [
                        "Extract methods to reduce complexity",
                        "Apply strategy pattern for conditional logic",
                        "Use guard clauses to reduce nesting"
                    ]
                    recommendations.append(recommendation)
            
            return recommendations
            
        except Exception as e:
            logger.error(f"Error generating performance recommendations: {e}")
            return []
    
    async def _generate_quality_recommendations(self, tree: ast.AST, code: str, 
                                              file_path: str) -> List[OptimizationRecommendation]:
        """Generate code quality improvement recommendations"""
        try:
            recommendations = []
            
            # Analyze quality
            quality_analysis = self.quality_analyzer.analyze_quality(code, file_path)
            
            # Generate recommendations for maintainability issues
            for issue in quality_analysis.maintainability_issues:
                priority = OptimizationPriority.HIGH if issue['severity'] == 'high' else OptimizationPriority.MEDIUM
                
                recommendation = OptimizationRecommendation(
                    optimization_type=OptimizationType.MAINTAINABILITY,
                    priority=priority,
                    strategy=OptimizationStrategy.REFACTOR,
                    target_file=file_path,
                    target_lines=(issue.get('line', 0), issue.get('line', 0)),
                    description=issue['description'],
                    reasoning=f"Maintainability issue: {issue['type']}",
                    expected_improvement={'maintainability': 0.25, 'readability': 0.15},
                    implementation_effort="Medium",
                    risk_assessment={'maintainability_risk': 0.05},
                    confidence_score=0.8,
                    learning_source="quality_pattern_analysis"
                )
                recommendation.testing_recommendations = [
                    "Add unit tests for refactored methods",
                    "Verify behavior unchanged after refactoring"
                ]
                recommendations.append(recommendation)
            
            # Generate recommendations for code smells
            for smell in quality_analysis.code_smells:
                recommendation = OptimizationRecommendation(
                    optimization_type=OptimizationType.CODE_QUALITY,
                    priority=OptimizationPriority.MEDIUM,
                    strategy=OptimizationStrategy.REFACTOR,
                    target_file=file_path,
                    target_lines=(smell.get('line', 0), smell.get('line', 0)),
                    description=smell['description'],
                    reasoning=f"Code smell detected: {smell['type']}",
                    expected_improvement={'maintainability': 0.2, 'readability': 0.2},
                    implementation_effort="Low to Medium",
                    risk_assessment={'maintainability_risk': 0.1},
                    confidence_score=0.7,
                    learning_source="code_smell_detection"
                )
                recommendations.append(recommendation)
            
            return recommendations
            
        except Exception as e:
            logger.error(f"Error generating quality recommendations: {e}")
            return []
    
    async def _generate_pattern_recommendations(self, tree: ast.AST, code: str, 
                                              file_path: str) -> List[OptimizationRecommendation]:
        """Generate design pattern recommendations"""
        try:
            recommendations = []
            
            # Get pattern recommendations
            pattern_recommendations = self.pattern_engine.recommend_patterns(tree, code)
            
            for pattern_rec in pattern_recommendations:
                if pattern_rec.applicability_score > 0.5:
                    recommendation = OptimizationRecommendation(
                        optimization_type=OptimizationType.DESIGN_PATTERN,
                        priority=OptimizationPriority.MEDIUM,
                        strategy=OptimizationStrategy.PATTERN_APPLICATION,
                        target_file=file_path,
                        description=f"Apply {pattern_rec.pattern_name} pattern",
                        reasoning=f"Pattern applicability score: {pattern_rec.applicability_score:.2f}",
                        expected_improvement={
                            'maintainability': 0.3,
                            'extensibility': 0.4,
                            'testability': 0.2
                        },
                        implementation_effort="Medium to High",
                        risk_assessment={'complexity_risk': 0.2},
                        confidence_score=pattern_rec.applicability_score,
                        learning_source="design_pattern_analysis"
                    )
                    
                    recommendation.alternative_solutions = pattern_rec.implementation_steps
                    recommendation.documentation_updates = [
                        f"Document {pattern_rec.pattern_name} pattern usage",
                        "Update architecture documentation"
                    ]
                    
                    recommendations.append(recommendation)
            
            return recommendations
            
        except Exception as e:
            logger.error(f"Error generating pattern recommendations: {e}")
            return []
    
    async def _generate_security_recommendations(self, tree: ast.AST, code: str, 
                                               file_path: str) -> List[OptimizationRecommendation]:
        """Generate security improvement recommendations"""
        try:
            recommendations = []
            
            # Security vulnerability patterns
            security_patterns = {
                'sql_injection': {
                    'pattern': r'execute\s*\(["\'].*%s.*["\']',
                    'description': 'Potential SQL injection vulnerability',
                    'severity': 'high'
                },
                'eval_usage': {
                    'pattern': r'eval\s*\(',
                    'description': 'Dangerous eval() usage',
                    'severity': 'high'
                },
                'hardcoded_secrets': {
                    'pattern': r'(password|api_key|secret)\s*=\s*["\'][^"\']+["\']',
                    'description': 'Hardcoded secrets detected',
                    'severity': 'medium'
                }
            }
            
            for pattern_name, pattern_info in security_patterns.items():
                matches = re.finditer(pattern_info['pattern'], code, re.IGNORECASE)
                for match in matches:
                    line_num = code[:match.start()].count('\n') + 1
                    
                    priority = OptimizationPriority.CRITICAL if pattern_info['severity'] == 'high' else OptimizationPriority.HIGH
                    
                    recommendation = OptimizationRecommendation(
                        optimization_type=OptimizationType.SECURITY,
                        priority=priority,
                        strategy=OptimizationStrategy.IMMEDIATE,
                        target_file=file_path,
                        target_lines=(line_num, line_num),
                        description=pattern_info['description'],
                        reasoning=f"Security vulnerability: {pattern_name}",
                        expected_improvement={'security': 0.8},
                        implementation_effort="Low",
                        risk_assessment={'security_risk': 0.9},
                        original_code=match.group(),
                        optimized_code=await self._generate_secure_alternative(pattern_name, match.group()),
                        confidence_score=0.9,
                        learning_source="security_pattern_analysis"
                    )
                    recommendations.append(recommendation)
            
            return recommendations
            
        except Exception as e:
            logger.error(f"Error generating security recommendations: {e}")
            return []
    
    async def _generate_optimized_code(self, bottleneck: Dict[str, Any]) -> str:
        """Generate optimized code for performance bottleneck"""
        try:
            bottleneck_type = bottleneck.get('type', '')
            original_code = bottleneck.get('code_snippet', '')
            
            if bottleneck_type == 'inefficient_loops':
                return original_code.replace('range(len(', 'enumerate(').replace('))', ')')
            elif bottleneck_type == 'string_concatenation':
                return "# Use join() or f-strings instead of += for string concatenation"
            elif bottleneck_type == 'repeated_calculations':
                return "# Cache calculation results outside the loop"
            else:
                return "# Optimized version would be generated based on specific bottleneck"
                
        except Exception as e:
            logger.error(f"Error generating optimized code: {e}")
            return "# Optimization suggestion would be generated"
    
    async def _generate_secure_alternative(self, vulnerability_type: str, original_code: str) -> str:
        """Generate secure alternative for security vulnerability"""
        try:
            if vulnerability_type == 'sql_injection':
                return "# Use parameterized queries: cursor.execute('SELECT * FROM table WHERE id = %s', (user_id,))"
            elif vulnerability_type == 'eval_usage':
                return "# Use ast.literal_eval() for safe evaluation or implement specific parsing"
            elif vulnerability_type == 'hardcoded_secrets':
                return "# Use environment variables: os.getenv('SECRET_KEY')"
            else:
                return "# Secure alternative would be provided"
                
        except Exception as e:
            logger.error(f"Error generating secure alternative: {e}")
            return "# Security improvement would be suggested"
    
    async def _update_learning_database(self, recommendations: List[OptimizationRecommendation], 
                                      file_path: str):
        """Update learning database with new recommendations"""
        try:
            # Simple learning mechanism - would be more sophisticated in production
            for recommendation in recommendations:
                pattern_key = f"{recommendation.optimization_type.value}_{recommendation.target_element}"
                
                if pattern_key not in self.learning_database:
                    self.learning_database[pattern_key] = {
                        'count': 0,
                        'success_rate': 0.0,
                        'avg_confidence': 0.0
                    }
                
                entry = self.learning_database[pattern_key]
                entry['count'] += 1
                entry['avg_confidence'] = (entry['avg_confidence'] + recommendation.confidence_score) / 2
            
        except Exception as e:
            logger.error(f"Error updating learning database: {e}")
    
    def get_optimization_summary(self, recommendations: List[OptimizationRecommendation]) -> Dict[str, Any]:
        """Get summary of optimization recommendations"""
        try:
            summary = {
                'total_recommendations': len(recommendations),
                'by_type': {},
                'by_priority': {},
                'expected_improvements': {},
                'implementation_effort_distribution': {},
                'confidence_distribution': {}
            }
            
            # Group by type
            for recommendation in recommendations:
                opt_type = recommendation.optimization_type.value
                summary['by_type'][opt_type] = summary['by_type'].get(opt_type, 0) + 1
            
            # Group by priority
            for recommendation in recommendations:
                priority = recommendation.priority.value
                summary['by_priority'][priority] = summary['by_priority'].get(priority, 0) + 1
            
            # Aggregate expected improvements
            improvement_totals = defaultdict(float)
            for recommendation in recommendations:
                for metric, value in recommendation.expected_improvement.items():
                    improvement_totals[metric] += value
            
            summary['expected_improvements'] = dict(improvement_totals)
            
            # Implementation effort distribution
            for recommendation in recommendations:
                effort = recommendation.implementation_effort
                summary['implementation_effort_distribution'][effort] = (
                    summary['implementation_effort_distribution'].get(effort, 0) + 1
                )
            
            # Confidence distribution
            high_confidence = len([r for r in recommendations if r.confidence_score > 0.8])
            medium_confidence = len([r for r in recommendations if 0.6 <= r.confidence_score <= 0.8])
            low_confidence = len([r for r in recommendations if r.confidence_score < 0.6])
            
            summary['confidence_distribution'] = {
                'high': high_confidence,
                'medium': medium_confidence,
                'low': low_confidence
            }
            
            return summary
            
        except Exception as e:
            logger.error(f"Error getting optimization summary: {e}")
            return {'error': str(e)}

# Factory function for creating intelligent code optimizer
def create_intelligent_code_optimizer() -> IntelligentCodeOptimizer:
    """Create and initialize intelligent code optimizer"""
    try:
        optimizer = IntelligentCodeOptimizer()
        logger.info("Intelligent Code Optimizer created successfully")
        return optimizer
    except Exception as e:
        logger.error(f"Error creating Intelligent Code Optimizer: {e}")
        raise

# Example usage and testing
async def main():
    """Example usage of Intelligent Code Optimizer"""
    try:
        # Create optimizer
        optimizer = create_intelligent_code_optimizer()
        
        # Example code to optimize
        example_code = '''
class DataManager:
    def __init__(self):
        self.data = []
        self.processed_data = []
    
    def process_large_dataset(self, dataset):
        """Process large dataset with multiple inefficiencies"""
        result = ""
        
        # Inefficient string concatenation
        for i in range(len(dataset)):
            item = dataset[i]
            
            # Nested loops - O(n²) complexity
            for j in range(len(dataset)):
                if dataset[j]['id'] == item['related_id']:
                    # Magic number
                    if item['value'] > 100:
                        result += f"Processing {item['name']} "
                        
                        # Repeated calculation
                        expensive_calc = self.expensive_operation(item)
                        if expensive_calc > 50:
                            result += "important "
        
        return result
    
    def expensive_operation(self, item):
        """Simulate expensive operation"""
        total = 0
        for i in range(1000):  # Magic number
            total += i * item.get('multiplier', 1)
        return total
    
    def validate_data_with_many_conditions(self, data):
        """Method with high cyclomatic complexity"""
        if data['type'] == 'A':
            return data['value'] > 10
        elif data['type'] == 'B':
            return data['value'] > 20
        elif data['type'] == 'C':
            return data['value'] > 30
        elif data['type'] == 'D':
            return data['value'] > 40
        elif data['type'] == 'E':
            return data['value'] > 50
        elif data['type'] == 'F':
            return data['value'] > 60
        else:
            return False
'''
        
        # Write example to temporary file
        temp_file = "temp_optimization_example.py"
        with open(temp_file, 'w') as f:
            f.write(example_code)
        
        try:
            # Perform optimization analysis
            recommendations = await optimizer.analyze_and_optimize(temp_file)
            
            # Get summary
            summary = optimizer.get_optimization_summary(recommendations)
            
            print("=== Intelligent Code Optimization Results ===")
            print(f"Total Recommendations: {summary['total_recommendations']}")
            print(f"By Type: {summary['by_type']}")
            print(f"By Priority: {summary['by_priority']}")
            print(f"Expected Improvements: {summary['expected_improvements']}")
            print(f"Confidence Distribution: {summary['confidence_distribution']}")
            
            # Show detailed recommendations
            print("\n=== Detailed Recommendations ===")
            for i, rec in enumerate(recommendations[:5], 1):  # Show first 5
                print(f"\n{i}. {rec.description}")
                print(f"   Type: {rec.optimization_type.value}")
                print(f"   Priority: {rec.priority.value}")
                print(f"   Strategy: {rec.strategy.value}")
                print(f"   Confidence: {rec.confidence_score:.3f}")
                print(f"   Expected Improvement: {rec.expected_improvement}")
                print(f"   Implementation Effort: {rec.implementation_effort}")
                if rec.optimized_code:
                    print(f"   Optimized Code: {rec.optimized_code}")
                if rec.alternative_solutions:
                    print(f"   Alternatives: {rec.alternative_solutions}")
        
        finally:
            # Cleanup
            if Path(temp_file).exists():
                Path(temp_file).unlink()
        
    except Exception as e:
        logger.error(f"Error in main: {e}")

if __name__ == "__main__":
    asyncio.run(main())